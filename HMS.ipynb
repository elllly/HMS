{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\" # you can also use tensorflow or torch\n",
    "\n",
    "import keras_cv\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import ops\n",
    "\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from tqdm.notebook import tqdm\n",
    "import joblib\n",
    "\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TensorFlow:\", tf.__version__)\n",
    "print(\"Keras:\", keras.__version__)\n",
    "print(\"KerasCV:\", keras_cv.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    verbose = 1  # Verbosity\n",
    "    seed = 42  # Random seed\n",
    "    preset = \"efficientnetv2_b2_imagenet\"  # Name of pretrained classifier\n",
    "    image_size = [400, 300]  # Input image size\n",
    "    epochs = 13 # Training epochs\n",
    "    batch_size = 64  # Batch size\n",
    "    lr_mode = \"cos\" # LR scheduler mode from one of \"cos\", \"step\", \"exp\"\n",
    "    drop_remainder = True  # Drop incomplete batches\n",
    "    num_classes = 6 # Number of classes in the dataset\n",
    "    fold = 0 # Which fold to set as validation data\n",
    "    class_names = ['Seizure', 'LPD', 'GPD', 'LRDA','GRDA', 'Other']\n",
    "    label2name = dict(enumerate(class_names))\n",
    "    name2label = {v:k for k, v in label2name.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.set_random_seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train + Valid\n",
    "df = pd.read_csv('train.csv')\n",
    "df['eeg_path'] = f'train_eegs\\\\'+df['eeg_id'].astype(str)+'.parquet'\n",
    "df['spec_path'] = f'train_spectrograms\\\\'+df['spectrogram_id'].astype(str)+'.parquet'\n",
    "df['spec2_path'] = f'train_spectrograms\\\\'+df['spectrogram_id'].astype(str)+'.npy'\n",
    "df['class_name'] = df.expert_consensus.copy()\n",
    "df['class_label'] = df.expert_consensus.map(CFG.name2label)\n",
    "display(df.head(2))\n",
    "\n",
    "# Test\n",
    "test_df = pd.read_csv('test.csv')\n",
    "test_df['eeg_path'] = f'test_eegs\\\\'+test_df['eeg_id'].astype(str)+'.parquet'\n",
    "test_df['spec_path'] = f'test_spectrograms\\\\'+test_df['spectrogram_id'].astype(str)+'.parquet'\n",
    "test_df['spec2_path'] = f'test_spectrograms\\\\'+test_df['spectrogram_id'].astype(str)+'.npy'\n",
    "display(test_df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to process a single eeg_id\n",
    "def process_spec(spec_id, split=\"train\"):\n",
    "    spec_path = f\"{split}_spectrograms\\{spec_id}.parquet\"\n",
    "    spec = pd.read_parquet(spec_path)\n",
    "    spec = spec.fillna(0).values[:, 1:].T # fill NaN values with 0, transpose for (Time, Freq) -> (Freq, Time)\n",
    "    spec = spec.astype(\"float32\")\n",
    "    np.save(f\"{split}_spectrograms\\{spec_id}.npy\", spec)\n",
    "\n",
    "# Get unique spec_ids of train and valid data\n",
    "spec_ids = df[\"spectrogram_id\"].unique()\n",
    "\n",
    "# Parallelize the processing using joblib for training data\n",
    "_ = joblib.Parallel(n_jobs=-1, backend=\"loky\")(\n",
    "    joblib.delayed(process_spec)(spec_id, \"train\")\n",
    "    for spec_id in tqdm(spec_ids, total=len(spec_ids))\n",
    ")\n",
    "\n",
    "# Get unique spec_ids of test data\n",
    "test_spec_ids = test_df[\"spectrogram_id\"].unique()\n",
    "\n",
    "# Parallelize the processing using joblib for test data\n",
    "_ = joblib.Parallel(n_jobs=-1, backend=\"loky\")(\n",
    "    joblib.delayed(process_spec)(spec_id, \"test\")\n",
    "    for spec_id in tqdm(test_spec_ids, total=len(test_spec_ids))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_augmenter(dim=CFG.image_size):\n",
    "    augmenters = [\n",
    "        keras_cv.layers.MixUp(alpha=2.0),\n",
    "        keras_cv.layers.RandomCutout(height_factor=(1.0, 1.0),\n",
    "                                     width_factor=(0.06, 0.1)), # freq-masking\n",
    "        keras_cv.layers.RandomCutout(height_factor=(0.06, 0.1),\n",
    "                                     width_factor=(1.0, 1.0)), # time-masking\n",
    "    ]\n",
    "    \n",
    "    def augment(img, label):\n",
    "        data = {\"images\":img, \"labels\":label}\n",
    "        for augmenter in augmenters:\n",
    "            if tf.random.uniform([]) < 0.5:\n",
    "                data = augmenter(data, training=True)\n",
    "        return data[\"images\"], data[\"labels\"]\n",
    "    \n",
    "    return augment\n",
    "\n",
    "\n",
    "def build_decoder(with_labels=True, target_size=CFG.image_size, dtype=32):\n",
    "    def decode_signal(path, offset=None):\n",
    "        # Read .npy files and process the signal\n",
    "        file_bytes = tf.io.read_file(path)\n",
    "        sig = tf.io.decode_raw(file_bytes, tf.float32)\n",
    "        sig = sig[1024//dtype:]  # Remove header tag\n",
    "        sig = tf.reshape(sig, [400, -1])\n",
    "        \n",
    "        # Extract labeled subsample from full spectrogram using \"offset\"\n",
    "        if offset is not None: \n",
    "            offset = offset // 2  # Only odd values are given\n",
    "            sig = sig[:, offset:offset+300]\n",
    "            \n",
    "            # Pad spectrogram to ensure the same input shape of [400, 300]\n",
    "            pad_size = tf.math.maximum(0, 300 - tf.shape(sig)[1])\n",
    "            sig = tf.pad(sig, [[0, 0], [0, pad_size]])\n",
    "            sig = tf.reshape(sig, [400, 300])\n",
    "        \n",
    "        # Log spectrogram \n",
    "        sig = tf.clip_by_value(sig, tf.math.exp(-4.0), tf.math.exp(8.0)) # avoid 0 in log\n",
    "        sig = tf.math.log(sig)\n",
    "        \n",
    "        # Normalize spectrogram\n",
    "        sig -= tf.math.reduce_mean(sig)\n",
    "        sig /= tf.math.reduce_std(sig) + 1e-6\n",
    "        \n",
    "        # Mono channel to 3 channels to use \"ImageNet\" weights\n",
    "        sig = tf.tile(sig[..., None], [1, 1, 3])\n",
    "        return sig\n",
    "    \n",
    "    def decode_label(label):\n",
    "        label = tf.one_hot(label, CFG.num_classes)\n",
    "        label = tf.cast(label, tf.float32)\n",
    "        label = tf.reshape(label, [CFG.num_classes])\n",
    "        return label\n",
    "    \n",
    "    def decode_with_labels(path, offset=None, label=None):\n",
    "        sig = decode_signal(path, offset)\n",
    "        label = decode_label(label)\n",
    "        return (sig, label)\n",
    "    \n",
    "    return decode_with_labels if with_labels else decode_signal\n",
    "\n",
    "\n",
    "def build_dataset(paths, offsets=None, labels=None, batch_size=32, cache=True,\n",
    "                  decode_fn=None, augment_fn=None,\n",
    "                  augment=False, repeat=True, shuffle=1024, \n",
    "                  cache_dir=\"\", drop_remainder=False):\n",
    "    if cache_dir != \"\" and cache is True:\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "    \n",
    "    if decode_fn is None:\n",
    "        decode_fn = build_decoder(labels is not None)\n",
    "    \n",
    "    if augment_fn is None:\n",
    "        augment_fn = build_augmenter()\n",
    "    \n",
    "    AUTO = tf.data.experimental.AUTOTUNE\n",
    "    slices = (paths, offsets) if labels is None else (paths, offsets, labels)\n",
    "    \n",
    "    ds = tf.data.Dataset.from_tensor_slices(slices)\n",
    "    ds = ds.map(decode_fn, num_parallel_calls=AUTO)\n",
    "    ds = ds.cache(cache_dir) if cache else ds\n",
    "    ds = ds.repeat() if repeat else ds\n",
    "    if shuffle: \n",
    "        ds = ds.shuffle(shuffle, seed=CFG.seed)\n",
    "        opt = tf.data.Options()\n",
    "        opt.experimental_deterministic = False\n",
    "        ds = ds.with_options(opt)\n",
    "    ds = ds.batch(batch_size, drop_remainder=drop_remainder)\n",
    "    ds = ds.map(augment_fn, num_parallel_calls=AUTO) if augment else ds\n",
    "    ds = ds.prefetch(AUTO)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=CFG.seed)\n",
    "\n",
    "df[\"fold\"] = -1\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "for fold, (train_idx, valid_idx) in enumerate(\n",
    "    sgkf.split(df, y=df[\"class_label\"], groups=df[\"patient_id\"])\n",
    "):\n",
    "    df.loc[valid_idx, \"fold\"] = fold\n",
    "df.groupby([\"fold\", \"class_name\"])[[\"eeg_id\"]].count().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from full data\n",
    "sample_df = df.groupby(\"spectrogram_id\").head(1).reset_index(drop=True)\n",
    "train_df = sample_df[sample_df.fold != CFG.fold]\n",
    "valid_df = sample_df[sample_df.fold == CFG.fold]\n",
    "print(f\"# Num Train: {len(train_df)} | Num Valid: {len(valid_df)}\")\n",
    "\n",
    "# Train\n",
    "train_paths = train_df.spec2_path.values\n",
    "train_offsets = train_df.spectrogram_label_offset_seconds.values.astype(int)\n",
    "train_labels = train_df.class_label.values\n",
    "train_ds = build_dataset(train_paths, train_offsets, train_labels, batch_size=CFG.batch_size,\n",
    "                         repeat=True, shuffle=True, augment=True, cache=True)\n",
    "\n",
    "# Valid\n",
    "valid_paths = valid_df.spec2_path.values\n",
    "valid_offsets = valid_df.spectrogram_label_offset_seconds.values.astype(int)\n",
    "valid_labels = valid_df.class_label.values\n",
    "valid_ds = build_dataset(valid_paths, valid_offsets, valid_labels, batch_size=CFG.batch_size,\n",
    "                         repeat=False, shuffle=False, augment=False, cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, tars = next(iter(train_ds))\n",
    "\n",
    "num_imgs = 8\n",
    "plt.figure(figsize=(4*4, num_imgs//4*5))\n",
    "for i in range(num_imgs):\n",
    "    plt.subplot(num_imgs//4, 4, i + 1)\n",
    "    img = imgs[i].numpy()[... ,0]  # Adjust as per your image data format\n",
    "    img -= img.min()\n",
    "    img /= img.max() + 1e-4\n",
    "    tar = CFG.label2name[np.argmax(tars[i].numpy())]\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"Target: {tar}\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOSS = keras.losses.KLDivergence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Classifier\n",
    "model = keras_cv.models.ImageClassifier.from_preset(\n",
    "    CFG.preset, num_classes=CFG.num_classes\n",
    ")\n",
    "\n",
    "# Compile the model  \n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "              loss=LOSS)\n",
    "\n",
    "# Model Sumamry\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def get_lr_callback(batch_size=8, mode='cos', epochs=10, plot=False):\n",
    "    lr_start, lr_max, lr_min = 5e-5, 6e-6 * batch_size, 1e-5\n",
    "    lr_ramp_ep, lr_sus_ep, lr_decay = 3, 0, 0.75\n",
    "\n",
    "    def lrfn(epoch):  # Learning rate update function\n",
    "        if epoch < lr_ramp_ep: lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n",
    "        elif epoch < lr_ramp_ep + lr_sus_ep: lr = lr_max\n",
    "        elif mode == 'exp': lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n",
    "        elif mode == 'step': lr = lr_max * lr_decay**((epoch - lr_ramp_ep - lr_sus_ep) // 2)\n",
    "        elif mode == 'cos':\n",
    "            decay_total_epochs, decay_epoch_index = epochs - lr_ramp_ep - lr_sus_ep + 3, epoch - lr_ramp_ep - lr_sus_ep\n",
    "            phase = math.pi * decay_epoch_index / decay_total_epochs\n",
    "            lr = (lr_max - lr_min) * 0.5 * (1 + math.cos(phase)) + lr_min\n",
    "        return lr\n",
    "\n",
    "    if plot:  # Plot lr curve if plot is True\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(np.arange(epochs), [lrfn(epoch) for epoch in np.arange(epochs)], marker='o')\n",
    "        plt.xlabel('epoch'); plt.ylabel('lr')\n",
    "        plt.title('LR Scheduler')\n",
    "        plt.show()\n",
    "\n",
    "    return keras.callbacks.LearningRateScheduler(lrfn, verbose=False)  # Create lr callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_cb = get_lr_callback(CFG.batch_size, mode=CFG.lr_mode, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_cb = keras.callbacks.ModelCheckpoint(\"best_model.keras\",\n",
    "                                         monitor='val_loss',\n",
    "                                         save_best_only=True,\n",
    "                                         save_weights_only=False,\n",
    "                                         mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_ds, \n",
    "    epochs=CFG.epochs,\n",
    "    callbacks=[lr_cb, ckpt_cb], \n",
    "    steps_per_epoch=len(train_df)//CFG.batch_size,\n",
    "    validation_data=valid_ds, \n",
    "    verbose=CFG.verbose\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"best_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_paths = test_df.spec2_path.values\n",
    "test_ds = build_dataset(test_paths, batch_size=min(CFG.batch_size, len(test_df)),\n",
    "                         repeat=False, shuffle=False, cache=False, augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = test_df[[\"eeg_id\"]].copy()\n",
    "target_cols = [x.lower()+'_vote' for x in CFG.class_names]\n",
    "pred_df[target_cols] = preds.tolist()\n",
    "\n",
    "sub_df = pd.read_csv(f'sample_submission.csv')\n",
    "sub_df = sub_df[[\"eeg_id\"]].copy()\n",
    "sub_df = sub_df.merge(pred_df, on=\"eeg_id\", how=\"left\")\n",
    "sub_df.to_csv(\"submission.csv\", index=False)\n",
    "sub_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
